
#include <webcraft/async/runtime.hpp>
#include <webcraft/async/executors.hpp>
#include <queue>
#include <thread>
#include <webcraft/concurrency/queue.hpp>
#include <random>
#include <iostream>

// basic executor implementation, which is a simple executor that yields control back to the runtime
class basic_executor : public webcraft::async::executor
{
private:
    webcraft::async::async_runtime &runtime;

public:
    basic_executor(webcraft::async::async_runtime &runtime) : runtime(runtime)
    {
    }

    webcraft::async::task<void> schedule(webcraft::async::scheduling_priority priority) override
    {
        return runtime.yield();
    }
};

// thread per task executor, highly inefficient but shows the idea of how we can combine both concurrency and parallelism
class thread_per_task : public webcraft::async::executor
{
public:
    thread_per_task() {}

    webcraft::async::task<void> schedule(webcraft::async::scheduling_priority priority) override
    {
        struct thread_per_task_awaitable
        {
            bool await_ready() { return false; }
            void await_suspend(std::coroutine_handle<> h)
            {
                std::thread([h]()
                            { h.resume(); })
                    .detach();
            }
            void await_resume() {}
        };

        co_await thread_per_task_awaitable{};
    }
};

class thread_pool : public webcraft::async::executor
{
public:
    struct worker
    {
        std::jthread thr;

        worker() = default;
        virtual ~worker()
        {
            stop();
        }

        virtual void runLoop(std::stop_token stop_token) = 0;

        void start()
        {
            thr = std::jthread([&](std::stop_token t)
                               { runLoop(t); });
        }

        void stop()
        {
            thr.request_stop();
        }
    };

protected:
    std::vector<std::unique_ptr<worker>> workers;

public:
    thread_pool() {}
    ~thread_pool()
    {
        stop();
    }

    virtual std::vector<std::unique_ptr<worker>> createWorkers() = 0;

    void start()
    {
        workers = createWorkers();
        for (auto &worker : workers)
        {
            worker->start();
        }
    }

    void stop()
    {
        for (auto &worker : workers)
        {
            worker->stop();
        }
    }
};

class fixed_size_thread_pool : public thread_pool
{
public:
    struct worker : public thread_pool::worker
    {
        fixed_size_thread_pool &pool;

        worker(fixed_size_thread_pool &pool) : pool(pool) {}

        void runLoop(std::stop_token stop_token) override
        {
            while (!stop_token.stop_requested())
            {
                std::function<void()> task;

                {
                    std::unique_lock<std::mutex> lock(pool.mutex);
                    pool.condition.wait(lock, [this, stop_token]
                                        { return !pool.tasks.empty() || stop_token.stop_requested(); });

                    if (stop_token.stop_requested())
                        return;

                    if (!pool.tasks.empty())
                    {
                        task = std::move(pool.tasks.front());
                        pool.tasks.pop_front();
                    }
                }

                if (task)
                {
                    task();
                }
            }
        }
    };

    std::mutex mutex;
    std::condition_variable condition;
    std::deque<std::function<void()>> tasks;
    size_t num_workers;

    fixed_size_thread_pool(size_t num_workers = std::thread::hardware_concurrency()) : num_workers(num_workers)
    {
        workers.resize(num_workers);
    }

    ~fixed_size_thread_pool()
    {
        stop();
        condition.notify_all();
    }

    std::vector<std::unique_ptr<thread_pool::worker>> createWorkers() override
    {
        std::vector<std::unique_ptr<thread_pool::worker>> created_workers;
        created_workers.reserve(num_workers);
        for (size_t i = 0; i < num_workers; ++i)
        {
            created_workers.emplace_back(std::make_unique<worker>(*this));
        }
        return created_workers;
    }

    webcraft::async::task<void> schedule(webcraft::async::scheduling_priority priority) override
    {
        struct fixed_size_awaitable
        {
            fixed_size_thread_pool &pool;

            bool await_ready() { return false; }

            void await_suspend(std::coroutine_handle<> h)
            {
                std::unique_lock<std::mutex> lock(pool.mutex);
                pool.tasks.emplace_back([h]()
                                        { h.resume(); });
                pool.condition.notify_one();
            }

            void await_resume() {}
        };

        co_await fixed_size_awaitable{*this};
        co_return;
    }
};

// Generated by ChatGPT o3
struct work_stealing_thread_pool : public thread_pool
{
    // TODO Implement work stealing logic onto it
    struct worker : public thread_pool::worker
    {
        work_stealing_thread_pool &pool;
        webcraft::concurrency::lock_free_deque<std::coroutine_handle<>, 16> task_queue;
        std::mt19937 rng_;
        std::uniform_int_distribution<std::size_t> dist_;

        explicit worker(work_stealing_thread_pool &p, std::size_t id)
            : pool(p), rng_(static_cast<std::uint32_t>(id * 0x9E3779B9u)), dist_(0, p.num_workers - 1) {}

        std::coroutine_handle<> steal_one()
        {
            for (int probes = 0; probes < 2 * static_cast<int>(pool.num_workers); ++probes)
            {
                std::size_t victim = dist_(rng_);
                if (victim >= pool.workers.size() || pool.workers[victim].get() == this)
                    continue; // self
                auto *w = static_cast<worker *>(pool.workers[victim].get());
                if (auto h = w->task_queue.pop_back())
                    return h.value();
            }
            return {};
        }
        ~worker() = default;

        void runLoop(std::stop_token token) override
        {
            // Expose this worker via thread_local so schedule() can locate us
            thread_local worker *tls_self = this;
            while (!token.stop_requested())
            {
                // 1️⃣ local pop
                auto h = task_queue.pop_front();
                // 2️⃣ steal others
                if (!h)
                    h = steal_one();
                // 3️⃣ global queue
                if (!h)
                    h = pool.dequeue();
                // 4️⃣ Park when no work
                if (!h)
                {
                    int expected = 1; // wait only when permit present
                    pool.sleep_flag_.wait(expected, std::memory_order_acquire);
                    pool.sleep_flag_.store(0, std::memory_order_relaxed); // consume permit
                    continue;
                }
                // Execute task – resume coroutine
                h.value().resume();
            }
            tls_self = nullptr;
        }
    };

    std::mutex mutex;
    std::atomic<int> sleep_flag_{0}; // 0 – awake / tasks present, 1 – sleepers may wait
    std::deque<std::coroutine_handle<>> tasks;
    size_t num_workers;

    work_stealing_thread_pool(size_t num_workers = std::thread::hardware_concurrency()) : num_workers(num_workers)
    {
    }

    std::vector<std::unique_ptr<thread_pool::worker>> createWorkers() override
    {
        std::vector<std::unique_ptr<thread_pool::worker>> created_workers;
        created_workers.reserve(num_workers);
        for (size_t i = 0; i < num_workers; ++i)
        {
            created_workers.emplace_back(std::make_unique<worker>(*this, i));
        }
        return created_workers;
    }

    void enqueue(std::coroutine_handle<> h)
    {
        {
            std::unique_lock<std::mutex> lock(mutex);
            tasks.push_back(h);
        }
        // If someone was sleeping set permit and notify
        sleep_flag_.store(1, std::memory_order_release);
        sleep_flag_.notify_one();
    }

    std::coroutine_handle<> dequeue()
    {
        std::lock_guard lk(mutex);
        if (tasks.empty())
            return {};
        auto h = tasks.front();
        tasks.pop_front();
        return h;
    }

    webcraft::async::task<void> schedule(webcraft::async::scheduling_priority priority) override
    {
        struct work_stealing_awaitable
        {
            work_stealing_thread_pool &pool;

            bool await_ready() { return false; }

            void await_suspend(std::coroutine_handle<> h)
            {
                thread_local worker *self = nullptr;
                if (self && self->task_queue.push_back(h))
                {
                }
                else
                {
                    pool.enqueue(h);
                }
            }

            void await_resume() {}
        };

        co_await work_stealing_awaitable{*this};
        co_return;
    }
};

webcraft::async::executor_service::executor_service(async_runtime &runtime, executor_service_params &params) : runtime(runtime)
{
    auto tp = std::make_unique<fixed_size_thread_pool>();
    tp->start();

    // TODO: implement the executor service strategy based on the params
    this->strategy = std::move(tp);
}

// TODO: implement these
webcraft::async::executor_service::~executor_service()
{
}
